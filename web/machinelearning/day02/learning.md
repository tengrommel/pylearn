# 算法分类
- 监督学习
  - 函数的输出可以是一个连续的值
  - 或是输出是有限个数离散值（称作分类）
- 无监督学习
- 半监督学习
- 强化学习

# 正则化
> 通过限制高次项的系数进行防止过拟合
- Lasso回归
> 直接把高次项前面的系数变为0
- 岭回归
> 把高次项系数前面的系数变成特别小的值

# 正则化线性模型

Ridge Regression 岭回归
   
   - 就是把系数前面添加平方项
   - 然后限制数值的大小
   - α值越小，系数值越大，α越大，系数值越小

Lasso回归

   - 对系数值进行绝对值处理
   - 由于绝对值在顶点处不可导，所以进行计算的过程中产生很多0，最后得到结果为：稀疏矩阵

弹性网络
> 混合了岭回归和Lasso 设置了一个r，如果r=0 为岭回归 如果r=1 为Lasso回归

Early Stopping

# 模型的保存和加载

api:
> sklearn.externals import joblib

# 逻辑回归
> 解决的是二分类的问题

- 输入是什么 线行驶回归的输出
- 原理
    - 线性回归的输出
    - 激活函数 把整体的值映射到[0, 1]
    - 在设置一个阈值，进行完成
    - 损失 对数似然损失
        - 借助log思想，进行完成
        - 真实值等于0，等于两种情况进行划分
    - 优化
        - 提升原本属于1类别的概率，降低原本是0的概率
- 如何判断逻辑回归的输出

# 精确率和召回率
 
# ROC曲线与AUC指标
> ROC曲线就是FPR TPR 绘制图像

# 决策树算法

是种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶子代表一种分类结果，本质是一颗由多个判断节点组成的树。

*熵 混乱程度*

# 信息增益
> 量化信息的重要程度

信息增益 = entroy(前) - entroy(后)

# 集成学习算法
> 集成学习通过建立几个模型来解决单一预测问题

# 集成学习中boosting和Bagging

Bagging

- 采样 从所有样本里面，采样一部分
- 学习  训练弱学习器
- 集成 使用平权投票

Boosting

# 随机森林
> 一个包含多个决策树的分类器，并且其输出的类别的众数而定

随机森林构造过程中的关键步骤

- 一次随机选出一个样本，有放回的抽样，重复N次
- 随机去选出m个特征，m<<M，建立决策树



