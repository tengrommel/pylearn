# 算法分类
- 监督学习
  - 函数的输出可以是一个连续的值
  - 或是输出是有限个数离散值（称作分类）
- 无监督学习
- 半监督学习
- 强化学习

# 正则化
> 通过限制高次项的系数进行防止过拟合
- Lasso回归
> 直接把高次项前面的系数变为0
- 岭回归
> 把高次项系数前面的系数变成特别小的值

# 正则化线性模型

Ridge Regression 岭回归
   
   - 就是把系数前面添加平方项
   - 然后限制数值的大小
   - α值越小，系数值越大，α越大，系数值越小

Lasso回归

   - 对系数值进行绝对值处理
   - 由于绝对值在顶点处不可导，所以进行计算的过程中产生很多0，最后得到结果为：稀疏矩阵

弹性网络
> 混合了岭回归和Lasso 设置了一个r，如果r=0 为岭回归 如果r=1 为Lasso回归

Early Stopping

# 模型的保存和加载

api:
> sklearn.externals import joblib

# 逻辑回归
> 解决的是二分类的问题

- 输入是什么 线行驶回归的输出
- 原理
    - 线性回归的输出
    - 激活函数 把整体的值映射到[0, 1]
    - 在设置一个阈值，进行完成
    - 损失 对数似然损失
        - 借助log思想，进行完成
        - 真实值等于0，等于两种情况进行划分
    - 优化
        - 提升原本属于1类别的概率，降低原本是0的概率
- 如何判断逻辑回归的输出


